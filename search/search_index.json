{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PDSA Notes","text":""},{"location":"#for-iitm-bs-degree","title":"For IITM BS Degree","text":""},{"location":"#important-links-for-the-course","title":"Important Links for the course","text":"<ul> <li>Course portal direct access</li> <li>Replit Team - <code>Join with IITM ID</code></li> <li>Youtube Course Playlist</li> <li>Live sessions - <code>Previous Terms</code></li> <li> <p>Grading Document</p> </li> <li> <p>Reference Books</p> </li> </ul> <p>Bug</p> <p>If you find a formatting issue in displaying math equations, kindly refresh the page. This is a bug and I am working on it. Hope to remove it soon.</p>"},{"location":"01%20Inro%20to%20Jupyter/","title":"Introduction to Jupyter Notebook","text":"<p>A basic Jupyter notebook is a sequence of cells. You can consider it like a sequence of one-dimensional spreadsheet.</p> <p>A cell basically holds a piece of code or documentation.</p> <p>You can edit and re-run code in a cell without disturbing the rest of program. </p> <p>It is the first choice for the ML Developers.</p>"},{"location":"01%20Inro%20to%20Jupyter/#why-use-jupyter-notebook","title":"Why use Jupyter Notebook?","text":""},{"location":"01%20Inro%20to%20Jupyter/#collaborative-by-default","title":"Collaborative by default","text":"<ul> <li> <p>You can share your code easily by the use of Jupyter notebooks along with the results.</p> </li> <li> <p>You also have the capability of writing code and documentation in the same file.</p> </li> </ul>"},{"location":"01%20Inro%20to%20Jupyter/#documentation","title":"Documentation","text":"<ul> <li>In the Jupyter notebook the code inter exists with the documentation..</li> </ul>"},{"location":"01%20Inro%20to%20Jupyter/#different-versions-of-your-code","title":"Different versions of your code","text":"<ul> <li>You can get a multiple versions of code and run the whole code bit by bit.</li> </ul>"},{"location":"01%20Inro%20to%20Jupyter/#easy-file-sharing","title":"Easy file sharing","text":"<ul> <li>A large number of supported file formats.</li> </ul>"},{"location":"01%20Inro%20to%20Jupyter/#supports-different-languages","title":"Supports different languages","text":"<ul> <li>It supports Julia, Python and R</li> </ul>"},{"location":"01%20Inro%20to%20Jupyter/#alternatives-for-jupyter-notebook","title":"Alternatives for Jupyter notebook","text":"<p>Frankly speaking, there are no real alternatives for Jupyter notebook.</p> <p>But if you don't want to install Jupyter notebook on your device you go also head on Google Colab</p>"},{"location":"01%20Inro%20to%20Jupyter/#advantages-of-google-colab","title":"Advantages of Google Colab","text":"<ul> <li>It is pre-customised for ML Development.</li> <li>It has all the major ML packages installed and configured. You are just to code and run to get the output.</li> <li>Your code executes on GPUs.</li> </ul>"},{"location":"02%20Revisiting%20python/","title":"Revisiting Python","text":""},{"location":"02%20Revisiting%20python/#some-common-programs","title":"Some common programs","text":""},{"location":"02%20Revisiting%20python/#computing-gcd-for-two-numbers-m-and-n","title":"Computing \"GCD\" for two numbers <code>m</code> and <code>n</code>.","text":"Approach 1<pre><code>def gcd(m, n):\n    # List of common factors of m and n\n    cf = []\n    for i in range(1, min(m, n) + 1):\n        if (m%i == 0) and (n%i == 0):\n          cf.append(i)\n\n    return (cf[-1])\n</code></pre> Approach 2<pre><code>def gcd(m, n):\n  # List of common factors of m and n\n  mrcf = 1\n  for i in range(1, min(m, n) + 1):\n      if (m%i == 0) and (n%i == 0):\n        mrcf = i\n\n  return (mrcf)\n</code></pre> <p>Here both the codes are proportional to <code>min(m, n)</code></p>"},{"location":"02%20Revisiting%20python/#recursive-approach","title":"Recursive Approach","text":"\\[ \\text{Let } d \\text { divides } m \\text{ and } n.\\\\ m = ad, n = bd\\\\ \\text{Now, } m - n = (a - b)d \\] <p>Therefore, d also divides <code>m</code> and <code>n</code> Using this we can also apply recursion to solve the GCD problem</p> Recursive-GCD.py<pre><code>def rec_gcd(m, n):\n  a, b = max(m, n), min(m, n)\n  if (a % b == 0):\n    return b\n  else:\n    return (rec_gcd(a, a-b))\n</code></pre>"},{"location":"02%20Revisiting%20python/#euclids-algorithm","title":"Euclid's algorithm","text":"<p>Suppose <code>n</code> does not divide <code>m</code> Then, let \\(m = nq + r ... (1)\\) Also, let \\(m =ad, n = bd ... (2)\\) Therefore eqn(\\(1\\)) becomes \\(ad + (bd)q + r\\). This means that \\(r\\) is of a form \\(cd\\).</p> <p>Now the recursive function becomes:</p> <ul> <li>Base case: m % n == 0, return n</li> <li>Recursive case: if m % n != 0, return gcd(m, m % n)</li> </ul> Optimised-GCD.py<pre><code>def opt_gcd(m, n):\n  a, b = max(m, n), min(m, n)\n  if (a % b == 0):\n    return b\n  else:\n    return (opt_gcd(b, a % b))\n</code></pre>"},{"location":"02%20Revisiting%20python/#checking-prime-numbers","title":"Checking prime numbers","text":"factors.py<pre><code>def factors(n):\n  f1 =[]\n  for i in range(1, n+1):\n    if n%i == 0:\n      f1.append(i)\n\n  return f1\n</code></pre> primes-approach-1.py<pre><code>def prime(n):\n  return (factors(n) == [1, n])\n</code></pre> primes-approach-2.py<pre><code>def prime(n):\n  return (len(factors(n)) == 2)\n</code></pre>"},{"location":"02%20Revisiting%20python/#listing-primes-upto-m","title":"Listing primes upto <code>m</code>","text":"primes-upto-m.py<pre><code>def primes_upto_m(m):\n  pr = []\n  for i in range(1, m+1):\n    if prime(i):\n      pr.append(i)\n\n  return pr\n</code></pre>"},{"location":"02%20Revisiting%20python/#listing-first-m-primes","title":"Listing first <code>m</code> primes","text":"first-m-primes.py<pre><code>def first_m_primes(m):\n  pr = []\n  i = 1\n  while (len(pr) &lt; m-1):\n    if prime(i):\n      pr.append(i)\n    i += 1\n\n  return pr\n</code></pre> For While When you know the number of iterations in advance. When you are not sure about the number of iterations in advance"},{"location":"02%20Revisiting%20python/#computing-primes-without-using-maintaining-factors-lists","title":"Computing primes without using maintaining factors lists","text":"prime-using-for-loop.py<pre><code>def prime(m):\n  flag = True\n  for i in range(2, n):\n    if m%i == 0:\n      flag = False\n      break\n\n  return flag\n</code></pre> prime-using-while-loop.py<pre><code>def prime(m):\n  flag = True\n  i = 2\n  while (flag &amp;&amp; i &lt; n)\n    if m%i == 0:\n      flag = False\n      break\n    i += 1\n\n  return flag\n</code></pre>"},{"location":"02%20Revisiting%20python/#optimising-prime-function","title":"Optimising prime function","text":"<p>We know that \\(\\sqrt m\\) is the middle factor of \\(m\\). Also factors occurs in pairs.</p> <p>So we can use this concept to check for primes with the limits to be <code>2</code> to \\(\\sqrt{m}\\) instead of <code>2</code> to <code>m</code></p> optimised-prime.py<pre><code>def prime(m):\n  flag = True\n  for i in range(2, int(n ** 0.5) + 1):\n    if m%i == 0:\n      flag = False\n      break\n\n  return flag\n</code></pre>"},{"location":"02%20Revisiting%20python/#assignments-to-try-out","title":"Assignments to try out","text":""},{"location":"02%20Revisiting%20python/#twin-primes","title":"Twin primes","text":"<p>Two prime numbers with their absolute difference to be 2 are called twin primes. Mathematically speaking, if \\(m\\) and \\(m+2\\) are primes then \\(m\\) and \\(m+2\\) are twin primes.</p>"},{"location":"03%20Exception%20Handling/","title":"Exception Handling in Python","text":"<p>Our code generates a number of errors. In this section, we are going to look out for them and how to avoid them.</p> <p>In Prof. Madhavan's words, we will learn how to Recover gracefully.</p>"},{"location":"03%20Exception%20Handling/#common-types-of-errors-while-a-program-is-syntactically-valid","title":"Common Types of Errors while a program is syntactically valid:","text":"Name of Error Cause NameError Variable used before initialising ZeroError A number is divided by zero IndexError List is out of range KeyError Key does not exist in dictionary"},{"location":"03%20Exception%20Handling/#using-try-except-block","title":"Using try-except block","text":"<p>Python provides a way to handle exceptions in the code using try-except block.</p> <pre><code>try:\n  # Some code to run here\nexcept errorName: # if you want to execute something on specific error\n  # Handle the specific error here.\nexcept: # Handles all the general errors\n  # General error handling\nelse:\n  # Code if your try block runs without any error\n</code></pre>"},{"location":"03%20Exception%20Handling/#using-exceptions-positively","title":"Using exceptions \"positively\"","text":"<p>Exceptions are of a great use and you can use them to make your program greatly readable and efficient.</p> <p>Example: Adding a new key in dictionary</p> <p>Let <code>D</code> be a dictionary whose values of lists and we want to add key <code>t</code> to it.</p> <pre><code>try:\n  D[t].append(\"abc\")\nexcept KeyError:\n  D[t] = [\"abc\"]\n</code></pre>"},{"location":"04%20Classes%20and%20Objects/","title":"Classes and Objects","text":"<p>To understand classes, we first need to know about abstract datatype.</p>"},{"location":"04%20Classes%20and%20Objects/#abstract-datatype","title":"Abstract Datatype","text":"<p>Let us consider an implementation of Stack in python language. </p> <p>We see that the Stack implementation itself gives functions to manipulate data, eg. <code>push()</code>or <code>pop()</code> and the implementation is private.</p> <p>So basically, an abstract datatype:</p> <ul> <li>Stores some data.</li> <li>Provides designated functions to manipulate data.</li> <li> Separates the (private) implementation and the (public) specification</li> </ul>"},{"location":"04%20Classes%20and%20Objects/#class","title":"Class","text":"<p>A implementation of is Abstract Datatype a class.</p> <p>A class is basically a:</p> <ul> <li>Template for a user defined datatype.</li> <li>How data is stored</li> <li>How public functions manipulate the data</li> </ul>"},{"location":"04%20Classes%20and%20Objects/#object","title":"Object","text":"<p>An object is an instance of a class.</p>"},{"location":"04%20Classes%20and%20Objects/#class-as-a-real-world-example","title":"Class as a real-world example","text":""},{"location":"04%20Classes%20and%20Objects/#2d-points","title":"2D Points","text":"<p>A class created in python always needs to have the `__init__()` function. It is a special function and is known as constructor. The only objective of the constructor is to initialise the data members of a class a default value.</p> <p>Each function of a class accepts a compulsory parameter known as <code>self</code>. It contains the a particular instance of a class to be worked upon. </p> <pre><code>class Point:\n  def __init__(self, a = 0, b = 0):\n    self.x = a\n    self.y = b\n</code></pre> <p>Let us define another function called translate that shifts the point \\((x,y)\\) by \\((\\Delta x, \\Delta y)\\).</p> <pre><code>def translate(self, deltax, deltay):\n  self.x += deltax\n  self.y += deltay\n</code></pre> <p>Let us define another function that gets us the distance of the point from origin using the formula \\(d=\\sqrt{x^2+y^2}\\)</p> <pre><code>def odistance(self):\n  d = ((x ** 2) + (y ** 2)) ** (0.5)\n  return d\n</code></pre> <p>Therefore the whole class <code>Point</code> is as follows:</p> <pre><code>class Point:\n  def __init__(self, a = 0, b = 0):\n    self.x = a\n    self.y = b\n\n  def translate(self, deltax, deltay):\n    self.x += deltax\n    self.y += deltay\n\n  def odistance(self):\n    d = ((x ** 2) + (y ** 2)) ** (0.5)\n    return d\n</code></pre>"},{"location":"04%20Classes%20and%20Objects/#special-class-functions","title":"Special Class functions","text":"Function name Use __init__() Constructor __str__() Converts object to string.  - It is implicitly called by <code>print</code> as print can only be used with strings. __add__() Implicitly called by <code>+</code> <p>Similarly there are many more functions such as <code>__mult__()</code> for multiplication, <code>__le__()</code> for less than operator and so on.</p>"},{"location":"05%20Timer/","title":"Timing our code","text":"<p>Sometimes, we need to know how much time does our code takes to execute.</p> <p>We can do that so in many ways, one of them being to use the <code>perf_counter()</code> function.</p> <p>The <code>perf_counter()</code> function gives us the current timestamp. It alone is not as useful but if we call the function at the start and the end of program and subtract both the timestamps we get the execution time of the code. </p>"},{"location":"05%20Timer/#using-perf_counter","title":"Using <code>perf_counter()</code>","text":"<p>We will create a class <code>Timer</code> to use this <code>perf_counter()</code> as this will help us to use this more and more.</p> Timer.py<pre><code>import time\nclass Timer:\n    def __init__(self):\n        self._start_time = None\n        self._elapsed_time = None\n\n    def start(self):\n        if self._start_time is not None:\n            raise Exception(\"Timer is already running\")\n        self._start_time = time.perf_counter()\n\n    def stop(self):\n        if self._start_time is None:\n            raise Exception(\"Timer is not running\")\n        end_time = time.perf_counter()\n        self._elapsed_time = end_time - self._start_time()\n        self._start_time = None\n\n    def __str__(self):\n        print(str(self._elapsed_time))\n</code></pre>"},{"location":"06%20Efficiency/","title":"Why Efficiency Matters","text":"<p>Our code needs to be efficient because in real world we need to work on a huge pile of data and our code needs to be efficient to save our resources and time.</p>"},{"location":"06%20Efficiency/#real-world-example","title":"Real World Example","text":"<p>A mobile network company needs to check whether each of their sim cards are linked to valid Aadhar card. Let us say, there are <code>N</code> sim cards and <code>M</code> Aadhar Cards.</p> <p>One of the naive approaches is:</p> Naive Approach<pre><code>foreach simcard S:\n    foreach Aadhar_number A:\n        check if AadharDetails of S matches A\n</code></pre> <p>Complexity of this approach = \\(M.N\\)</p>"},{"location":"06%20Efficiency/#problem-with-the-naive-approach","title":"Problem with the naive approach","text":"<p>Now let us consider the case of India: There are almost \\(10^9\\) Aadhar cards and \\(10^9\\) Sim cards.</p> <p>Now the complexity will be \\(10^{18}\\).</p> <p>As we know python can do \\(10^7\\) operations in <code>1 sec</code>.</p> <p>This operation will therefore take \\(\\frac{10^{18}}{10^7} = 10^{11}\\text{secs}\\) \\(\\approx 3200\\text{ years}\\).</p> <p>To not get into this trap of nested loops, we need our code to be efficient which can be achieved by approaching the problem in another way.</p>"},{"location":"06%20Efficiency/#efficient-approach","title":"Efficient Approach","text":"<p>Assuming, Aadhar numbers list is sorted, one of the way is to use <code>Binary Search</code> to search for the aadhar number.</p> <p>By halving the search for aadhar numbers even 10 times, reduces the time by \\(2^{10} = 1024\\) times.</p> <p>After 10 queries, the aadhar card space shrinks to \\(10^6\\).</p> <p>After 20 queries, the aadhar card space shrinks to \\(10^3\\).</p> <p>After 30 queries, the aadhar card space shrinks to \\(1\\).</p> <p>Therefore, time taken by the program = \\(10^9 * 30 \\approx 50 \\text{ minutes}\\).</p> <p>Tip</p> <p>Hence, \"Efficiency matters.\"</p>"},{"location":"07%20Analysis%20of%20Algorithms/","title":"Analysis of Algorithms","text":"<p>We measure the performance of an algorithm in terms of the amount of resources it consumes. The most important resource is time, but other resources such as memory consumption, network usage, storage usage, etc. are also important.</p> <p>The two main criteria to measure the efficiency of an algorithm are:</p> <ul> <li>Running Time: The amount of time an algorithm takes to complete.</li> <li>Space: The amount of memory/storage an algorithm uses.</li> </ul> <p>In general, we focus on time rather than space, because memory is cheap and abundant, but time is not.</p>"},{"location":"07%20Analysis%20of%20Algorithms/#measuring-running-time","title":"Measuring Running Time","text":"<p>The running time depends on:</p>"},{"location":"07%20Analysis%20of%20Algorithms/#input-size","title":"Input Size","text":"<p>Running time depends on the size of the input. For example, sorting 10 elements takes less time than sorting 1000 elements in an array.</p> <p>Measure the running time as a function of the input size, i.e., if the input size is <code>n</code>, then the running time is a function of <code>n</code>, i.e., <code>t(n)</code>.</p> <p>Warning</p> <p>Different inputs of the same size <code>n</code> may take different amounts of time to execute.</p>"},{"location":"07%20Analysis%20of%20Algorithms/#asymptotic-complexity","title":"Asymptotic Complexity","text":"<p>When comparing <code>t(n)</code> focus on the orders of magnitude and ignore the constant factors.</p> <p>We find the asymptotic complexity of an algorithm by finding the order of magnitude of the running time function <code>t(n)</code>.</p> <p>Mathematically, we find the \\(lim_{n\\to\\infin} t(n)\\).</p> <p>Here is a list of most common input sizes and their orders of magnitude based on the <code>t(n)</code>:</p> <p></p> Input Size Complexity"},{"location":"07%20Analysis%20of%20Algorithms/#which-inputs-to-consider","title":"Which Inputs to Consider","text":"<p>Performance of an algorithm can vary across different input instances.</p> <p>By luck, we can accomplish the task at the start itself or the algorithm may need to run down through the whole input and maybe still not accomplish the task, e.g., finding an element that does not exist in the input.</p> <p>Hence, we consider the worst case where the input forces the algorithm to take longest possible time to complete. By doing this. we may know the maximum time that the algorithm requires and hence establish an upper bound.</p>"},{"location":"07%20Analysis%20of%20Algorithms/#some-points-to-note","title":"Some Points to Note","text":"<ul> <li>Analysis should be independent of the hardware and software environment.</li> <li>Don't use actual time.</li> <li>Measure the number of operations executed by the algorithm.</li> </ul>"},{"location":"08%20Comparing%20Orders%20of%20Magnitude/","title":"Comparing Orders of Magnitude","text":"<p>This section deals with how to compare two functions based on their orders of magnitude.</p>"},{"location":"08%20Comparing%20Orders%20of%20Magnitude/#big-o-notationupper-bounds","title":"Big-O Notation(Upper Bounds)","text":"<p>\\(f(x)\\) is said to be \\(O(g(x))\\) if there exists two constants \\(c\\) and \\(x_0\\) such that the function \\(c.g(x)\\) is an upper bound for \\(f(x)\\) beyond \\(x_0\\).</p> <p>Mathematically,</p> \\[ f(x) \\leq c.g(x), \\text{for every } x\\geq x_0 \\] <p>Warning</p> <p>There can be more than a pair of \\(x_0\\) and \\(c\\) for every algorithm.</p>"},{"location":"08%20Comparing%20Orders%20of%20Magnitude/#properties-of-big-o-notation","title":"Properties of Big-O Notation","text":""},{"location":"08%20Comparing%20Orders%20of%20Magnitude/#addition-of-two-functions","title":"Addition of two functions","text":"<p>The addition of two functions lies below the maximum of the two functions in terms of Big-O notation.</p> \\[ \\text{Let }f_1(n)\\text{ is }O(g_1(n)) \\\\ \\ \\\\ \\text{and }f_2(n)\\text{ is }O(g_2(n))\\\\ \\ \\\\ \\text{then } f_1(n)+f_2(n)\\text{ is }O(max(g_1(n), g_2(n))). \\] <p>Tip</p> <p>The upper bound of any algorithm is given by the least efficient phase of the algorithm.</p>"},{"location":"08%20Comparing%20Orders%20of%20Magnitude/#omega-notationlower-bounds","title":"\\(\\Omega\\) notation(Lower Bounds)","text":"<p>\\(f(x)\\) is said to be \\(\\Omega(g(x))\\) if there exists two constants \\(c\\) and \\(x_0\\) such that the function \\(c.g(x)\\) is a lower bound for \\(f(x)\\) beyond \\(x_0\\).</p> <p>Mathematically,</p> \\[ f(x) \\geq c.g(x), \\text{for every } x\\geq x_0 \\] <p>Note</p> <p>Typically we establish lower bounds for whole problem as a whole, rather than each algorithm of it.</p>"},{"location":"08%20Comparing%20Orders%20of%20Magnitude/#theta-notationtight-bounds","title":"\\(\\Theta\\) notation(Tight Bounds)","text":"<p>\\(f(x)\\) is said to be \\(\\Theta(g(x))\\) if there exists three constants \\(c_1,c_2\\) and \\(x_0\\) such that the function \\(c_1.g(x)\\) is a lower bound for \\(f(x)\\) beyond \\(x_0\\) and \\(c_2.g(x)\\) is an upper bound for \\(f(x)\\) beyond \\(x_0\\).</p> \\[ c_1.g(x) \\leq f(x) \\leq c_2.g(x), \\text{for every } x\\geq x_0 \\]"},{"location":"09%20Calculating%20Complexity/","title":"Calculating complexity","text":""},{"location":"09%20Calculating%20Complexity/#iterative-programs","title":"Iterative Programs","text":""},{"location":"09%20Calculating%20Complexity/#example-1-maximum-from-a-list","title":"Example-1 (Maximum from a list)","text":"<p>Consider the following function</p> max_list.py<pre><code>def max_elem(L):\n    max_val = L[0]\n    for i in L:\n        if i &gt; max_val:\n            max_val = i\n\n    return max_val\n</code></pre> <ul> <li>Input size is the length of <code>L</code>.</li> <li>Always take <code>n</code>, i.e., length of <code>L</code> steps to complete.</li> <li>Worst case Complexity is \\(O(n)\\).</li> </ul>"},{"location":"09%20Calculating%20Complexity/#example-2-check-whether-the-loop-has-duplicates","title":"Example-2 (Check whether the loop has duplicates)","text":"duplicates_in_list.py<pre><code>    def hasDuplicates(L):\n        n = len(L)\n        for i in range(n):\n            for j in range(i+1, n):\n                if L[i] == L[j]:\n                    return False\n        return True\n</code></pre> <ul> <li>Input size is len of <code>L</code>, i.e., <code>n</code></li> <li>Worst case is that no duplicates exist and both the loops execute completely.</li> <li>Time is \\((n-1)+(n-2)+(n-3)+\\dots+1=\\cfrac{n(n-1)}{2}\\)</li> <li>Worst case complexity is \\(O(n^2)\\)</li> </ul>"},{"location":"09%20Calculating%20Complexity/#example-3-matrix-multiplication","title":"Example-3 (Matrix Multiplication)","text":"matrix_multiplication.py<pre><code>def multiply(A, B):\n    m, n, p = len(A), len(B), len(B[0])\n\n    C = [[0 for i in range(p)]\n        for j in range(m)]\n\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i][j] += A[i][k] * B[k][j]\n\n    return C\n</code></pre> <ul> <li>Input size: Input matrices has the size \\(m \\text{ x } n,n\\text{ x }p\\).</li> <li>Output matrix is \\(m\\text{ x }p\\).</li> <li>Overall time is \\(O(mnp)\\), i.e., \\(O(n^3)\\), if \\(m = p = n\\).</li> </ul>"},{"location":"09%20Calculating%20Complexity/#example-4-count-number-of-bits-in-binary-representation-of-a-number-n","title":"Example-4 (Count number of bits in binary representation of a number <code>n</code>)","text":"no_of_bits.py<pre><code>def countBits(n):\n    count = 1\n    while n &gt; 1:\n        count += 1\n        n //= 2\n    return count\n</code></pre> <ul> <li>This problem takes \\(\\log(n)\\) steps to reach \\(1\\) from \\(n\\).</li> </ul> <p>Tip</p> <p>For number theoretic problems, input size is the number of digits in the number.</p>"},{"location":"09%20Calculating%20Complexity/#recursive-programs","title":"Recursive Programs","text":""},{"location":"09%20Calculating%20Complexity/#example-1-towers-of-hanoi","title":"Example-1 (Towers of Hanoi)","text":"<p>Tower of Hanoi is a mathematical puzzle where we have three rods (A, B, and C) and N disks. Initially, all the disks are stacked in decreasing value of diameter i.e., the smallest disk is placed on the top and they are on rod A. The objective of the puzzle is to move the entire stack to another rod (here considered C), obeying the following simple rules:</p> <ul> <li>Only one disk can be moved at a time.</li> <li>Each move consists of taking the upper disk from one of the stacks and placing it on top of another stack i.e. a disk can only be moved if it is the uppermost disk on a stack.</li> <li>No disk may be placed on top of a smaller disk.</li> </ul> <p>Read more about Towers of Hanoi.</p> <p>To solve this problem we use a recursive approach. You can learn more about the approach from the Proffessor's video.</p>"},{"location":"09%20Calculating%20Complexity/#approach","title":"Approach","text":"<ul> <li>Let the initial peg be <code>A</code>, final peg be <code>B</code> and transit peg be <code>C</code>.</li> <li>Move \\(n-1\\) discs from from peg <code>A</code> to <code>C</code>.</li> <li>Then move the last disc from <code>A</code> to <code>B</code>.</li> <li>Now <code>C</code> has \\(n-1\\) discs and <code>B</code> has 1 disc.</li> <li>Now move \\(n-2\\) discs from <code>C</code> to <code>A</code>. And the last disc from <code>C</code> to <code>B</code> and so on.</li> </ul> \\[ \\text{Now let } M(n) - \\text{ Number of moves to move }n\\text{ discs and }M(1)=1. \\\\\\ \\\\ \\text{So, using the recursive approach, }\\\\\\ \\\\M(n) = M(n-1)+M(1)+M(n-1) = 2M(n-1)+1 \\] <p>Similarly if you unwind the \\(M(n-1)\\) and so on, then \\(M(n) = 2^{n-1}+(2^{n-1}-1) = 2^{n-1}\\)</p>"},{"location":"10%20Searching%20in%20a%20list/","title":"Searching in a list","text":""},{"location":"10%20Searching%20in%20a%20list/#naive-approach","title":"Naive Approach","text":"<ul> <li>Scan the whole list for the element required.</li> <li>Worst case is when the element is not present in the list.</li> <li>Worst case time complexity is \\(O(n)\\), where \\(n\\) is the size of the input list.</li> </ul> <pre><code>def naiveSearch(L, k):\n    for i in L:\n        if i == k:\n            return True\n    return False\n</code></pre>"},{"location":"10%20Searching%20in%20a%20list/#binary-search-approach","title":"Binary Search Approach","text":"<ul> <li>The only condition for binary search is that the list should be sorted.</li> </ul>"},{"location":"10%20Searching%20in%20a%20list/#approach","title":"Approach","text":"<ul> <li>Let <code>k</code> be the element to be searched and the list to be sorted in ascending order.</li> <li>Compare <code>k</code> with the mid point of the list.</li> <li>If it matches, return <code>True</code></li> <li>If the mid value is \\(&gt;\\) <code>k</code>, then search the first half, else search the second half.</li> <li>Stop when the interval to be searched becomes empty</li> </ul> <pre><code>def binarySearch(L, k):\n    beg, end = 0, len(L)\n    while beg != end:\n        mid = (beg + end) // 2\n        if L[mid] == k:\n            return True\n        if L[mid] &lt; k:\n            beg = mid + 1\n        else:\n            end = mid - 1\n    return False\n</code></pre> <ul> <li>This apprach takes \\(\\log(n)\\) time to search for an element at a worst case scenario.</li> </ul>"},{"location":"10%20Searching%20in%20a%20list/#activity","title":"Activity","text":"<p>Implement the binary search algorithm using recursion</p>"},{"location":"11%20Sorting%20Lists/","title":"Sorting Lists","text":""},{"location":"11%20Sorting%20Lists/#selection-sort","title":"Selection Sort","text":""},{"location":"11%20Sorting%20Lists/#algorithm","title":"Algorithm","text":"<ul> <li>Go through the list and find the smallest element and swap it with the first element.</li> <li>Next go through the rest of the list and find the smallest element and swap it with the second element.</li> <li>An so on...</li> </ul>"},{"location":"11%20Sorting%20Lists/#code","title":"Code","text":"<pre><code>def SelectionSort(L):\n    n = len(L)\n    if n &lt; 1:\n        return L\n\n    for i in range(n):\n        mpos = i\n        for j in range(i+1, n):\n            if L[j] &lt; L[mpos]:\n                mpos = j\n        L[i], L[mpos] = L[mpos], L[i]\n\n    return L\n</code></pre>"},{"location":"11%20Sorting%20Lists/#efficiency","title":"Efficiency","text":"<ul> <li>Outer loop takes \\(n\\) steps</li> <li>Inner loop takes \\(n - i\\) steps</li> <li>\\(T(n) = n + (n-1)+(n-2) +\\dots +1 = \\cfrac{n(n+1)}2\\)</li> <li>Hence, \\(T(n) = O(n^2)\\)</li> </ul>"},{"location":"11%20Sorting%20Lists/#insertion-sort","title":"Insertion Sort","text":""},{"location":"11%20Sorting%20Lists/#algorithm_1","title":"Algorithm","text":""},{"location":"11%20Sorting%20Lists/#approach-1","title":"Approach 1","text":"<ul> <li>Take the first element of the list and add it to a new list.</li> <li>Take the next element of the list and add it to list in such a way that the list remains sorted.</li> <li>Again take the next element of the list and add it to list in such a way that the list remains sorted.</li> <li>Repeat this until the orignal list is empty.</li> </ul>"},{"location":"11%20Sorting%20Lists/#approach-2","title":"Approach 2","text":"<ul> <li>Take the second element of the list and see if the element is smaller than the first element. If it is set the element to occupy the first position and push the list forward. If it is not leave the element and continue to next element.</li> <li>Now check whether the element is smaller than the any of the previous elements. If it is set the element to the correct position and push the list forward. If it is not leave the element and continue to next element.</li> <li>Repeat the steps until the list is empty.</li> </ul>"},{"location":"11%20Sorting%20Lists/#code_1","title":"Code","text":"<pre><code>def InsertionSort(L):\n    n = len(L)\n    if L &lt; 1:\n        return L\n    for i in range(n):\n        j = i\n        while (j &gt; 0) and (L[j] &lt; L[j - 1]):\n            L[j], L[j - 1] = L[j - 1], L[j]\n            j -= 1\n\n    return L\n</code></pre>"},{"location":"11%20Sorting%20Lists/#efficiency_1","title":"Efficiency","text":"<ul> <li>Outer loop takes \\(n\\) steps</li> <li>Inner loop takes \\(i\\) steps to insert <code>L[i]</code> into <code>L[:i]</code></li> <li>\\(T(n) = 0 + 1 + ... (n-1) = \\cfrac{n(n-1)}2\\)</li> <li>Hence, \\(T(n) = O(n^2)\\)</li> </ul>"},{"location":"11%20Sorting%20Lists/#merge-sort","title":"Merge Sort","text":""},{"location":"11%20Sorting%20Lists/#approach","title":"Approach","text":"<ul> <li>Divide the list into two halves</li> <li>Seprately sort the two halves, say <code>A</code> and <code>B</code>.</li> <li>Now compare the first elements of <code>A</code> and <code>B</code> and add the smaller element to the new list.</li> <li>Repeat untill the end of both the lists.</li> </ul> <p>Tip</p> <p>We use the principle of divide and conquer to solve the problem.</p>"},{"location":"11%20Sorting%20Lists/#code_2","title":"Code","text":"<pre><code>def merge(A, B):\n    m, n = len(A), len(B)\n    C, i, j, k = [], 0, 0, 0\n    while k &lt; m+n:\n        if i == m:\n            C.append(B[j: ])\n            k += (n - j)\n        elif j == n:\n            C.append(A[i:])\n            k += (m - i)\n        elif A[i] &lt; B[j]:\n            C.append(A[i])\n            i += 1\n            k += 1\n        else:\n            C.append(B[j])\n            j += 1\n            k += 1\n    return C\n\ndef mergeSort(L):\n    n = len(L)\n\n    if n &lt;= 1:\n        return L\n\n    left = mergeSort(L[:n//2])\n    right = mergeSort(L[n//2:])\n\n    B = merge(left, right)\n\n    return B\n</code></pre>"},{"location":"11%20Sorting%20Lists/#efficiency_2","title":"Efficiency","text":"<ul> <li>In the worst case, the loop in <code>merge</code> function runs \\(m+n\\) times.</li> <li>Hence <code>merge</code> function takes \\(O(m+n)\\) time.</li> <li>We know that, \\(O(m+n) = O(2(\\max(m, n)))\\). But since here \\(m\\approx n\\), hence the <code>merge</code> takes \\(O(n)\\) time.</li> <li>Now let \\(n\\) be the input size for <code>mergeSort</code>. And also \\(n=2^k\\).</li> <li>We can see that \\(T(0) = T(1) = 1\\) and \\(T(n) = 2T(n/2) + n\\), where \\(n\\) is the time required to merge the two arrays.</li> <li>Using <code>unwind and solve</code> we get, \\(T(n)=2^kT(\\cfrac{n}{2^k})+kn\\)</li> <li>Now since \\(k=\\log(n)\\), \\(T(n) = n + n\\log(n)\\)</li> <li>Hence \\(T(n) = O(n\\log(n))\\)</li> </ul>"},{"location":"11%20Sorting%20Lists/#quick-sort","title":"Quick Sort","text":""},{"location":"11%20Sorting%20Lists/#approach_1","title":"Approach","text":"<ul> <li>Select an element known as <code>pivot</code> and split the list with respect to the <code>pivot</code> say <code>m</code>.</li> <li>Then move all values \\(\\leq m\\) to the left of <code>L</code> and the values \\(\\geq m\\) to the right of <code>L</code>.</li> <li>Then we recursively sort both the halves.</li> </ul>"},{"location":"11%20Sorting%20Lists/#code_3","title":"Code","text":"<pre><code>def quickSort(L, l, r):\n    if (r - l == 1):\n        return L\n\n    pivot, lower, upper = L[l], l+1, l+1\n\n    for i in range(l+1, r):\n        if L[i] &gt; pivot:\n            upper += 1\n        else:\n            L[i], L[lower] = L[lower], L[i]\n            lower, upper = lower + 1, upper + 1\n    L[l], L[lower-1] = L[lower-1], L[l]\n    lower = lower - 1\n\n    quickSort(L, l, lower)\n    quickSort(L, lower + 1, upper)\n\n    return L\n</code></pre>"},{"location":"11%20Sorting%20Lists/#efficiency_3","title":"Efficiency","text":"<ul> <li>Partition with respect to <code>pivot</code> takes \\(O(n)\\) time.</li> <li>For the worst case, <code>pivot</code> is maximum or minimum, i.e. partitions are of sizes \\(0 \\text{ and } n-1\\)</li> <li>\\(T(n) = T(n-1) + n = O(n^2)\\)</li> <li>Worst case for quicksort is an array that is already sorted.</li> </ul>"},{"location":"TODO/","title":"TODO","text":""},{"location":"TODO/#add","title":"ADD","text":""},{"location":"TODO/#week-3","title":"Week-3","text":"<ul> <li>All lectures (except Lecture 3.1 and 3.2)</li> </ul>"},{"location":"Extras/01%20Binary%20Trees/","title":"Binary Trees","text":"<p>A binary tree is a special type of tree, where each node of a tree has atmost 2 child nodes, called the left node and the right node, respectively and order matters.</p> <p>One more important property is that except the root of the tree has a unique parent.</p> <p></p> Binary Tree"},{"location":"Extras/01%20Binary%20Trees/#important-notations-in-binary-tree","title":"Important Notations in Binary Tree","text":"<ul> <li>Size of a tree: Number of nodes</li> <li>Height of a tree: Number of edges in the longest path of the tree or Number of levels of the tree with the level of root equal to 0.</li> <li>Leaf nodes: A node that has no children.</li> </ul>"},{"location":"Week-4/01%20Properties%20of%20Graphs/","title":"Graphs","text":""},{"location":"Week-4/01%20Properties%20of%20Graphs/#directed-graphs","title":"Directed Graphs","text":""},{"location":"Week-4/01%20Properties%20of%20Graphs/#undirected-graphs","title":"Undirected Graphs","text":""},{"location":"Week-4/01%20Properties%20of%20Graphs/#properties-of-graphs","title":"Properties of Graphs","text":"<ul> <li>A graph with \\(n\\) vertices can have at most degree \\((n-1)\\).</li> <li>The sum of degrees in a graph should always sum up to be an even number.</li> <li>If two degrees in a graph are \\(n-1\\), then there will be no vertex with degree \\(1\\).</li> </ul>"},{"location":"Week-4/02Breadth%20First%20Search/","title":"Breadth First Search (BFS)","text":"<p>BFS is a graph traversal algorithm that starts traversing the graph from the root node and explores all the neighbouring nodes at the same depth. Then, it selects the nearest node and explores all the unexplored nodes. The algorithm follows the same process for each of the nearest nodes until it finds the goal.</p> <p>We use the data structure queue to implement BFS.</p>"},{"location":"Week-4/02Breadth%20First%20Search/#algorithm","title":"Algorithm","text":"<ul> <li>Step 1: Choose the root node and add it to the queue.</li> <li>Step 2: Mark it as visited. (You can make a list named <code>visited</code> to keep track of the visited nodes.)</li> <li>Step 3: Now remove the element you are currently in from the queue and add all of its neighours that are not visited to the queue.</li> <li>Step 4: Mark all the neighbours as visited.</li> <li>Step 5: Repeat steps 3 and 4 until the queue is empty.</li> </ul>"},{"location":"Week-4/04%20Topological%20Sorting/","title":"Topological Sorting","text":"<p>Topological sorting can only be done in Directed Acyclic Graphs (DAGs).</p> <p>It is a linear ordering of vertices such that for every directed edge \\(u \\rightarrow v\\), vertex \\(u\\) comes before \\(v\\) in the ordering.</p>"},{"location":"Week-4/04%20Topological%20Sorting/#steps-to-accomplish-topological-sort","title":"Steps to accomplish Topological Sort","text":"<ol> <li>Find the indegree of each node of the graph.</li> <li>Put the nodes with indegree \\(0\\) inside the Queue.</li> <li>Now remove the first element of the Queue, add it to the result list and remove the node along with its corresponding edges from the graph.</li> <li>Repeat steps \\(1,\\ 2\\) and \\(3\\) until the Queue is empty.</li> </ol>"},{"location":"Week-5/01-Shortest-path-in-weighted-graphs/","title":"Shortest Path in Weighted Undirected Graphs","text":""},{"location":"Week-5/01-Shortest-path-in-weighted-graphs/#weighted-graphs","title":"Weighted Graphs","text":"<p>The graphs that have some weights or costs on traversing through edges are called weighted graphs.</p>"},{"location":"Week-5/01-Shortest-path-in-weighted-graphs/#representation-of-a-weighted-graph","title":"Representation of a Weighted Graph","text":"<ul> <li>By Diagrams:</li> </ul> Weighted Graph Example <ul> <li>By Adjancency matrix: Weighted graphs are represented by \\(n\\text{ x }n\\) matrix where each element is represented as \\((d,w)\\), where \\(d\\) is destination vertex and \\(w\\) is the weight between the source and destination.Also, the element in \\(n^{th} \\text{row and }j^{th}\\text{ column}\\) represents an edge from \\(j\\to i\\) with the weight \\(w.\\)</li> </ul>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/","title":"Single Source Shortest Path","text":"<p>By single source shortest path, we mean that we need to find the shortest path from a specific source vertex to all other vertices.</p> <p>We can achieve the solution in two ways.</p>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#djikstras-algorithm","title":"Djikstra's Algorithm","text":""},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#algorithm","title":"Algorithm","text":"<ol> <li> <p>Initialize an empty \\(n\\ \\text x\\ n\\) matrix and in the first column set the distance  of the source vertex to be \\(0\\) and rest other vertices to \\(\\infin\\)</p> </li> <li> <p>Find the vertex \\(v\\) with the least distance \\(d.\\) Mark \\(v\\) as visited.</p> </li> <li> <p>Check the neighbours of \\(v\\) and check whether they are unvisited. If the neighbout is not visited, then check for \\(d[v]+W&lt;d[n].\\) If it is the case, then assign this value to \\(d[n].\\) </p> </li> <li> <p>Repeat steps (3) and (4) for all the unvisited vertices.</p> </li> </ol>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#implementation","title":"Implementation","text":""},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#using-a-numpy-array","title":"Using a NumPy Array","text":"<pre><code>def DjikstraAlgorithm(adj, source):\n    rows, cols, x = adj.shape\n    visited = {k: False for k in adj[rows]}\n    distance = {k: float('inf') for k in adj[rows]}\n\n    distance[source] = 0\n\n    for u in range(rows):\n        next_dist = min([distance[v] for v in range(rows)\n                           if not visited[v]\n                        ])\n        next_vertex_list = [v for v in range(rows)\n                              if not visited[v]\n                              and distance[v] == next_dist \n                           ]\n\n        if not next_vertex_list:\n            break\n\n        next_vertex = min(next_vertex_list)\n        visited[next_vertex] = True\n        for i in range(cols):\n            if (adj[next_vertex, i, 0] == 1) and (not visited[i]):\n                if dist[i] &gt; adj[next_vertex, i, 1] + next_dist:\n                    dist[i] = adj[next_vertex, i, 1]\n\n    return distance \n</code></pre>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#using-an-adjacency-list","title":"Using an Adjacency List","text":"<pre><code>def DjikstrasAlgorithm(adjList, source):\n    vertices = len(adjList.keys())\n    visited = {k: False for k in range(vertices)}\n    distance = {k: float('inf') for k in range(vertices)}\n\n    distance[source] = 0\n\n    for i in adjList:\n        next_dist = min([distance[v] for v in adjList \n                                        if not visited[v]\n                        ])\n        next_vertex_list = [v for v in adjList \n                                if not visited[v]\n                                and distance[v] == next_dist\n                           ]\n\n        if not next_vertex_list:\n            break\n\n        next_vertex = min(next_vertex_list)\n        visited[next_vertex] = True\n\n        for v in adjList:\n            for neighbour, weight in v:\n                if weight + next_dist &lt; distance[neighbour]:\n                    distance[neighbour] = weight + next_dist\n\n    return distance \n</code></pre>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#time-complexity","title":"Time Complexity","text":"<p>The time complexity for Djikstra's Algorithm in either of the two cases is \\(O(n^2).\\) </p> <p>You can minimise this using Heaps as discussed in further articles.</p> <p>Danger</p> <p>Djikstra's Algorithm does not allow the presence of negative edge weights in the graph.</p>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<p>Bellman Ford Algorithm is an algorithm to find Single Source Shortest Paths similar to the Djikstra's Algorithm except the fact that it allows the use of negative edge weights but not a negative cycle.</p>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#algorithm_1","title":"Algorithm","text":"<ol> <li>Initialise a dictionary \\(D\\) that stores the distance and \\(D(v) = \\begin{bmatrix}0 \\text{, If v is source vertex}\\\\ \\infin \\text{, otherwise}\\end{bmatrix}\\)</li> <li>Pick a vertex \\(j\\) that has an edge \\((j,k)\\) and set \\(D(k) = min(D(k), D(j)+\\text{weight}(j, k))\\)</li> <li>Repeat the step \\(2\\) for \\(n-1\\) times.</li> </ol>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#implementation_1","title":"Implementation","text":""},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#using-numpy-array","title":"Using Numpy Array","text":"<pre><code>def BellmanFord(adj, source):\n    rows, cols, x = adj.shape\n    distance = {k: float('inf') for k in range(rows)}\n\n    distance[source] = 0\n\n    for i in range(rows):\n        for u in range(rows):\n            for j in range(cols):\n                if adj[i, j, 0] == 1:\n                    distance[j] = min(distance[j], distance[i] + adj[i, j, 1])\n\n    return distance\n</code></pre>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#using-adjancency-list","title":"Using Adjancency List","text":"<pre><code>def BellmanFord(adj, source):\n    vertices = len(adj.keys())\n\n    distance = {k: float('inf') for k in range(vertices)}\n    distance[source] = 0\n\n    for i in adj:\n        for u in adj:\n            for vertex, dist in adj[u]:\n                distance[vertex] = min(distance[vertex], distance[u] + dist)\n\n    return distance\n</code></pre>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#time-complexity_1","title":"Time Complexity","text":"<ul> <li>Using the numpy method the time complexity is \\(O(n^3)\\)</li> <li>Shifting to the Adjacency List the time complexity becomes \\(O(mn),\\) where \\(m\\) is the number of edges and \\(n\\) is the number of vertices.</li> </ul>"},{"location":"Week-5/02-Single%20Source%20Shortest%20Path/#visualisation-of-single-source-shortest-path-algorithms","title":"Visualisation of Single Source Shortest Path Algorithms","text":""},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/","title":"All Pair Shortest Paths","text":"<p>Sometimes it is needed to find shortest path from each vertex to each vertex. </p> <p>For example, when laying railway lines it is important to find the shortest routes between pair of two cities.</p> <p>One of the ways to approach this problem is by running the single source shortest path algorithms on each and every node.</p> <p>But this is not efficient.</p> <p>This brings Floyd Warshall's algorithm into the picture.</p> <p>But before that we need to know about Warshall's Algorithm</p>"},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#warshalls-algorithm","title":"Warshall's algorithm","text":"<p>Consider a case where there is an edge from vertex \\(i\\) to \\(k,\\) i.e., \\(i\\to k.\\) And there also exists a vertex from \\(j\\) to \\(k,\\) i.e. \\(j\\to k.\\)</p> <p>Now because of this, I can reach \\(j\\) from \\(i\\) after travelling through the intermediate node \\(k,\\) i.e., \\(i\\to k\\to j.\\)</p>"},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#some-special-notations","title":"Some Special Notations","text":"<p>Consider B is a matrix, such that \\(B[i][j]\\) is \\(1\\) if there exists a path between the vertex \\(i\\) and \\(j\\)</p> <p>Now we denote all this cases using some special notations: - \\(B^0[i,j] = 1 \\leftrightarrow\\) there exists a direct edge from \\(i\\) to \\(j.\\) - \\(B^1[i,j] = 1 \\leftrightarrow\\) there exists a edge from \\(i\\) to \\(j\\) with maximum \\(1\\) intermediate node in between. - Similarly, \\(B^k[i,j] = 1 \\leftrightarrow\\) there exists a edge from \\(i\\) to \\(j\\) with maximum \\(k\\) intermediate nodes in between</p>"},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#floyd-warshalls-algorithm","title":"Floyd Warshall's Algorithm","text":""},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#algorithm","title":"Algorithm","text":"<ol> <li>Let us define a \\(n\\text{ x }n\\) matrix named \\(SP,\\) where \\(n\\) is the number of vertices.</li> <li>Initialise \\(SP^0[i,j]=W[i,j],\\) if there exists a direct edge from \\(i\\) to \\(j\\) and the weight of the edge is \\(W[i,j].\\)</li> <li>If there does not exist a direct edge between \\(i\\) and \\(j\\) set \\(SP^0[i,j] = \\infin\\)</li> <li>Now \\(SP^{k+1}[i,j] = \\min(SP^k[i,j], SP^k[i,k]+SP^k[k,j])\\) </li> <li>Repeat step \\(4\\) for n times.</li> <li>Now \\(SP^n[i,j]\\) contains the shortest distance between \\(i\\) and \\(j\\)</li> </ol>"},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#implementation","title":"Implementation","text":"<pre><code>def FloydWarshall(adj):\n    rows, cols, x = adj.shape\n\n    SP  = np.zeroes(shape=(rows, cols, cols+1))\n\n    for row in range(rows):\n        for col in range(cols):\n            SP[row, col, 0] = float('inf')\n\n    for row in range(rows):\n        for col in range(cols):\n            if adj[row, col, 0] == 1:\n                SP[row, col, 0] = adj[row, col, 1]\n\n    for k in range(cols + 1):\n        for row in range(rows):\n            for col in range(cols):\n                SP[i, j, k] = min(SP[i, j, k], SP[i, k-1, k-1] + SP[k-1, j, k-1] ) \n\n    return SP[:,:,cols]\n</code></pre>"},{"location":"Week-5/03-All%20Pair%20Shortest%20Paths/#time-complexity","title":"Time Complexity","text":"<p>This implementation has the time complexity of \\(O(n^3)\\)</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/","title":"Minimum Cost Spanning Tree","text":"<p>Sometime there may arise a case that we want to limit cost on a resource but want the graph to remain well-connected.</p> <p>For example, suppose a fiber connection company wants to connect \\(n\\) cities and they want to use the lowest amount of optic cable to be utilised.</p> <p>The solution to this kind of problems is Minimum Cost Spanning Tree. </p> <p>Tip</p> <p>A minimally connected graph is called a Spanning tree.</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#important-notes-on-spanning-trees","title":"Important Notes on Spanning Trees","text":"<ol> <li>Removing an edge from the tree disconnects a graph</li> <li>Adding an edge to the tree forms a loop.</li> <li>There can be more than one spanning tree for a graph</li> <li>A tree on \\(n\\) vertices has exactly \\(n-1\\) edges</li> <li>In a tree, each vertex is joined by a unique path.</li> </ol> <p>Warning</p> <p>We choose the spanning tree with minimum cost to solve the problem discussed at the start of the page.</p> <p>Now to create or find a minimum cost spanning tree, there are two algorithms, - Prim's algorithm - Kruskal's Algorithm</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#prims-algorithm","title":"Prim's Algorithm","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#algorithm","title":"Algorithm","text":"<ol> <li>Consider two sets \\(TE\\) and \\(TV,\\) representing tree edges in MCST and tree vertices in MCST respectively.</li> <li>Initially, \\(TV=TE=\\phi\\)</li> <li>Suppose we find an edge \\(e\\) with the lowest weight in the entire graph and it connects edges \\(i\\) and \\(j.\\)</li> <li>Now \\(TV\\) becomes \\(\\{i,j\\}\\) and \\(TE\\) becomes \\(\\{e\\}\\)</li> <li>Now find an edge \\(t\\) connecting vertices \\(v_1\\) and \\(v_2\\) such that it has the minimum weight and \\(v_1\\in TV\\) and \\(v_2 \\notin TV.\\)</li> <li>Now add \\(v_2\\) in \\(TV\\) and \\(t\\) in \\(TE.\\)</li> <li>Repeat steps \\(5\\) and \\(6\\) for \\(n-2\\) times</li> </ol>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#implementation","title":"Implementation","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#basic-implementation","title":"Basic Implementation","text":"<pre><code>def Primsalgorithm(adjList):\n    visited = {k: False for k in adjList}\n    distance = {k: float('inf') for k in adjList}\n    tree = []\n\n    visited[0] = True\n\n    for vertex, dist in adjList[0]:\n        distance[vertex] = dist\n\n    for i in adjList:\n        mindist = float('inf')\n        nextv = None\n\n        for ver, dist in adjList[i]:\n            if (visited[i]) and (not visited[ver]) and (dist &lt; mindist):\n                mindist = dist\n                nexte = (i, ver)\n                nextv = ver\n\n        if not nextv:\n            break\n\n        visited[nextv] = True\n        tree.append(nexte) \n\n        for ver, dist in adjList[nextv]:\n            if not visited[ver]:\n                distance[ver] = min(distance[ver], dist)\n\n    return tree\n</code></pre>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#a-better-approach","title":"A Better Approach","text":"<pre><code>def Primsalgorithm(adjList):\n    visited, distance = {}, {}\n    neighbour = {}\n\n    for k in adjList:\n        visited[k] = False\n        distance[k] = float('inf')\n        neighbour[k] = -1\n\n    visited[0] = True\n    for ver, dis in adjList[0]:\n        distance[ver] = dis\n        neighbour[ver] = 0\n\n    for i in range(1, len(adjList)):\n        nextd = min([distance[v] for v in adjList if not visited[v]])\n\n        next_vertex_list = [v for v in adjList if (not visited[v]) and (nextd == distance[v])]\n\n        if not next_vertex_list:\n            break\n\n        next_vertex = min(next_vertex_list)\n        visited[next_vertex] = True\n\n        for ver, dis in adjList[next_vertex]:\n            if not visited[ver]:\n                distance[ver] = min(distance[ver], dis)\n                neighbour[ver] = next_vertex\n\n    return neighbour\n</code></pre>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#time-complexity","title":"Time Complexity","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#basic-implementation_1","title":"Basic Implementation","text":"<p>The time complexity of the basic implementation is \\(O(n^3)\\)</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#approach-inspired-by-djikstras-algorithm","title":"Approach inspired by Djikstra's Algorithm","text":"<p>The second approach is inspired by the Djikstra's Algorithm. </p> <p>This implementation has the time complexity of \\(O(n^2)\\) similar to that of Djikstra's Algorithm</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#kruskals-algorithm","title":"Kruskal's Algorithm","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#algortihm","title":"Algortihm","text":"<ol> <li>Consider \\(n\\) component, i.e., \\(n\\) vertices.</li> <li>After ordering the edges in ascending order, pick the smallest edge and add it in the tree, if it does not forms a cycle.</li> <li>Repeat steps \\(1\\) and \\(2\\) untill you found \\(n-1\\) edge.</li> </ol>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#implementation_1","title":"Implementation","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#naive-implementation","title":"Naive Implementation","text":"<pre><code>def KruskalsAlgorithm(adjList):\n    component = {k: k for k in adjList}\n    tree = []\n    edges = []\n    for i in adjList:\n        for ver, dis in adjList[i]:\n            edges.extend([(i, ver, dis)])\n\n    edges = list(sorted(edges, key=lambda edge: edge[2]))\n\n    for source, dest, dis in edges:\n        if component[source] != component[dest]:\n            tree.append((source, dest))\n            c = component[source]\n            for v in adjList:\n                if component[v] == c:\n                    component[v] = component[dest]\n</code></pre>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#using-unionfind-data-structure","title":"Using UnionFind Data Structure","text":"<p>Assume <code>UnionFind</code> has be defined in the code written below:</p> <pre><code>def KruskalsAlgorithm(adjList):\n    components = UnionFind()\n    components.MakeUnionFind(adjList.keys())\n    tree = []\n    edges = []\n    for i in adjList:\n        for ver, dis in adjList[i]:\n            edges.extend([(i, ver, dis)])\n\n    edges = list(sorted(edges, key=lambda edge: edge[2]))\n\n    for source, dest, dis in edges:\n        if component.find(source) != component.find(dest):\n            tree.append((source, dest))\n            component.union(component.find(source), component.find(dest))\n</code></pre>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#time-complexity_1","title":"Time Complexity","text":""},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#basic-implementation_2","title":"Basic Implementation","text":"<p>The current implementation has the time complexity of \\(O(n^2)\\)</p>"},{"location":"Week-5/04%20Minimum%20Cost%20Spanning%20Tree/#using-union-find-data-structure","title":"Using Union Find Data Structure","text":"<p>The overall time complexity is $O((m+n)\\log(n)), $ where \\(m\\) is the number of edges which is atmost \\(n^2\\) so time complexity becomes \\(O(n \\log (n))\\)</p>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/","title":"Union Find Data Structure","text":"<p>The Union Find data structure has mainly 3 operations: - MakeUnionFind(\\(S\\)) - set up singleton components \\(s, s\\in S\\) - find(\\(s\\)) - find the component that contains element \\(s\\)  - union(\\(s,s'\\)) - merge two components \\(s\\) and \\(s'\\)</p>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#basic-setup-or-notations","title":"Basic Setup (or Notations)","text":"<p>Let us consider a set \\(S\\) partitioned into the components \\(\\{c_1,c_2,...c_j\\}\\) with only a single element \\(s\\in S,\\) i.e., each \\(s\\) belongs to a single component</p>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#implementation","title":"Implementation","text":""},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#naive-implementation","title":"Naive Implementation","text":"<ul> <li>Let us assume \\(S=\\{0,1,2,...,n-1\\}\\)</li> <li> <p>Create a dictionary <code>component</code></p> </li> <li> <p>Now for the function <code>MakeUnionFind(S)</code> - Set <code>component[i] = i</code> for each \\(i\\in S\\)</p> </li> <li> <p>For the function <code>find(i)</code> - return <code>component[i]</code></p> </li> <li>And for the function <code>union(i, j)</code> - set all the \\(s\\) from the component \\(i\\) to \\(j\\)</li> </ul> <pre><code>class UnionFind:\n    def __init__(self):\n        self.component = {}\n\n    def MakeUnionFind(self, S):\n        self.component = {i: i for i in S}\n        self.n = len(S)\n\n    def find(self, i):\n        return self.component[i]\n\n    def union(self, i, j):\n        c_old = self.components[i]\n        c_new = self.components[j]\n        for k in range(self.n):\n            if component[k] == c_old:\n                component[k] = c_new\n</code></pre>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#a-better-implementation","title":"A better Implementation","text":"<p>As we saw above, the <code>union</code> function was inefficient. </p> <p>So to improve that, we could create two more dictionaries <code>members</code> and <code>sizes</code>, where <code>members[c] = list of all members of the component c</code> and <code>sizes[c] = len(members[c])</code></p> <pre><code>class UnionFind:\n    def __init__(self):\n        self.component = {}\n        self.members = {}\n        self.sizes = {}\n\n    def MakeUnionFind(self, S):\n        for i in S: \n            self.component[i] = i\n            self.members[i] = [i]\n            self.size[i] = 1\n        self.n = len(S)\n\n    def find(self, i):\n        return self.component[i]\n\n    def union(self, i, j):\n        c_old = components[i]\n        c_new = components[j]\n\n        if self.size[c_old] &gt; self.size[c_new]:\n            return union(self, j, i)\n\n        for k in members[i]:\n            self.components[k] = c_new\n            self.members[j].append(k)\n            self.size[c_new] += 1\n\n        self.members[i] = []\n        self.size[c_old] = 0\n</code></pre>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#time-complexity","title":"Time Complexity","text":""},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#basic-implementation","title":"Basic Implementation","text":"<p>The time complexity of the funtions are: -  <code>MakeUnionFind</code> - \\(O(n)\\) - <code>find(i)</code> - \\(O(1)\\) - <code>union(i,j)</code> - \\(O(n)\\)</p>"},{"location":"Week-6/01%20Union%20Find%20Data%20Structure/#a-better-implementation_1","title":"A Better Implementation","text":"<p>The time complexity of the funtions are: -  <code>MakeUnionFind</code> - \\(O(n)\\) - <code>find(i)</code> - \\(O(1)\\) - <code>union(i,j)</code> - \\(O(\\text{size(Smaller Set)})\\) - Average Cost of \\(m\\) <code>union</code> operations - \\(O(m\\log m)\\)</p>"},{"location":"Week-6/02%20Priority%20Queues/","title":"Priority Queues","text":"<p>Sometimes, it becomes necessary to suspend everything that is running, and complete the high priority tasks first.</p> <p>The problem here is that the new tasks can come up dynamically, and we need to process them at the time of execution. </p> <p>To solve this problem, we define a new data structure known as Priority Queue</p>"},{"location":"Week-6/02%20Priority%20Queues/#operations-required","title":"Operations required","text":"<p>The priority queue needs to have two main functions: - <code>delete_max()</code> or <code>delete_min()</code> - This function identifies the smallest or largest element and remove it. - <code>insert()</code> - This function adds a new item to the queue based on the priorities.</p>"},{"location":"Week-6/02%20Priority%20Queues/#assumption","title":"Assumption","text":"<p>We will assume that a total of \\(N\\) entries enter and leave the priority queue</p>"},{"location":"Week-6/02%20Priority%20Queues/#strategy","title":"Strategy","text":""},{"location":"Week-6/02%20Priority%20Queues/#structure-of-priority-queue","title":"Structure of priority queue","text":"<ul> <li>It is a \\(\\sqrt{N}\\text{ x }\\sqrt{N}\\) array.</li> <li>Each row is in sorted order.</li> <li>We keep track of <code>size</code> of each row</li> </ul>"},{"location":"Week-6/02%20Priority%20Queues/#strategy-for-insert","title":"Strategy for <code>insert()</code>","text":"<p>To insert any element in the priority queue, we find the first row that has space, i.e., we find the first row whose <code>size !=</code> \\(\\sqrt{N}\\) and insert the element such that the row remains in sorted order.</p>"},{"location":"Week-6/02%20Priority%20Queues/#time-complexity","title":"Time Complexity","text":"<p>The time complexity of this approach is: - \\(O(\\sqrt{N})\\) time for finding the row with a space - \\(O(\\sqrt{N})\\) time for inserting the element inside the row with free space</p> <p>So the asymptotic time complexity of this function is \\(O(\\sqrt{N})\\)</p>"},{"location":"Week-6/02%20Priority%20Queues/#strategy-for-delete_max-or-delete_min","title":"Strategy for <code>delete_max()</code> or <code>delete_min()</code>","text":"<p>We have defined the priority queue to have all the rows sorted. So we make use of it here, when we want to delete maximum or minimum element.</p> <ol> <li>We can take the <code>maximum</code> or <code>minimum</code> element from each row </li> <li>Then we find the <code>maximum</code> or <code>minimum</code> of all those element obtained from step \\(1\\) </li> <li>Then update the size of the row from where the element is deleted from step \\(2\\)</li> </ol>"},{"location":"Week-6/02%20Priority%20Queues/#time-complexity_1","title":"Time complexity","text":"<p>The time complexity of this function is: - \\(O(1)\\) for finding the largest or smallest element in the row - \\(O(\\sqrt{N})\\) for finding the element to be deleted - \\(O(1)\\) to delete the element</p> <p>So the asymptotic time complexity of this function is \\(O(\\sqrt{N})\\)</p>"},{"location":"Week-6/02%20Priority%20Queues/#time-complexity-of-the-priority-queue","title":"Time complexity of the <code>Priority Queue</code>","text":"<p>So for the priority queue, </p> <ul> <li>Insert function has the time complexity of \\(O(\\sqrt{N})\\)</li> <li>Delete function has the time complexity of \\(O(\\sqrt{N})\\)</li> </ul> <p>Now assume that we start with an empty Priority Queue, then to add and delete \\(N\\) entries, the total time complexity is \\(O(N\\sqrt{N})\\)</p>"},{"location":"Week-6/03%20Heaps/","title":"Heaps","text":"<p>The heaps are tree implementations of the Priority Queues</p> <p>A heap is basically a Binary Tree with some more constraints.</p>"},{"location":"Week-6/03%20Heaps/#constraints","title":"Constraints","text":"<ul> <li> <p>Structural Constraint: The binary tree must be filled level by level, i.e., before completing level \\(n-1\\) you cannot move to level \\(n.\\)</p> </li> <li> <p>Constraint on values: For the max-heap, the children of each node must be either less than or equal to the parent node and for the min-heap, the children of each node must be either greater than or equal to the parent node. </p> </li> <li> <p>By the previous property, the root of the max-heap is the greatest element, and the root of the min-heap is the least element.</p> </li> </ul>"},{"location":"Week-6/03%20Heaps/#strategy","title":"Strategy","text":"<p>Now the priority queue requires two main functions,  - <code>insert(n)</code> - <code>delete_max()</code> or <code>delete_min()</code></p> <p>So, this topic focuses on how are we to implement this functions,</p>"},{"location":"Week-6/03%20Heaps/#insertn","title":"<code>insert(n)</code>","text":"<ol> <li>Add a new node, using the structural constraint of the heap and set the value of node to be <code>n</code>.</li> <li>Now check whether the node satisfies the value constraint and if not, swap the parent and the child node.</li> <li>Repeat \\((2)\\) by following the path to the root until the value constraint is satisfied.</li> </ol>"},{"location":"Week-6/03%20Heaps/#delete_max-or-delete_min","title":"<code>delete_max()</code> or <code>delete_min()</code>","text":"<p>Let us consider the case of a max-heap, </p> <p>So the maximum value of the tree will be the root of the tree.</p> <p>Now due to the structural constraint of the root, the node removed will be the last node of the tree. Doing so, the value of the last node will have no node to sit on.</p> <p>So what we do is take the value of the deleted node and set it as the value of root node and then do the steps, i.e., comparing the root with both child node and swapping it with the bigger child, until the value constraint of the heap is satisfies.</p>"},{"location":"Week-6/03%20Heaps/#implementation","title":"Implementation","text":""},{"location":"Week-6/03%20Heaps/#storing-the-tree","title":"Storing the tree","text":"<p>Consider the heap,  </p> <p>Store the heap as a list, i.e., <code>heap=[h0,h1,h2...h9]</code>. </p> <p>If you tinker upon the heap, you'll find that, children of <code>H[i]</code> will be <code>H[2i+1]</code>, <code>H[2i+2]</code> and the parent of <code>H[i]</code> will be <code>H[(i-1)//2]</code></p>"},{"location":"Week-6/03%20Heaps/#building-a-heap-from-a-list-heapify","title":"Building a heap from a list (<code>heapify()</code>)","text":""},{"location":"Week-6/03%20Heaps/#simple-approach","title":"Simple approach","text":"<p>One thing you can do, is start with an empty heap and do <code>insert(element_from_list)</code> repeatedly into the heap.</p> <p>The cost of this function is \\(O(N\\log(N))\\) for \\(N\\) inserts</p>"},{"location":"Week-6/03%20Heaps/#a-better-approach","title":"A better approach","text":"<p>If you consider a list, and consider it's leaf nodes, you will find that the leaf nodes are already satisfying the heap properties.</p> <p>And the number of leaf_nodes will be <code>n//2</code>, where <code>n</code> is the number of nodes</p> <p>So what you can do is start from the bottom of the list to be considered as heap, and start fixing it from the bottom, you will see that you can fix the whole list in \\(O(n)\\)</p>"},{"location":"Week-6/03%20Heaps/#insertn_1","title":"<code>insert(n)</code>","text":""},{"location":"Week-6/03%20Heaps/#time-complexity","title":"Time Complexity","text":""},{"location":"Week-6/03%20Heaps/#insertn_2","title":"<code>insert(n)</code>","text":"<p>Consider a max-heap, then for a <code>n</code>, such that \\(n&gt;\\) (the root value), there will be comparisons equal to the height of the tree, hence the complexity of inserting a node will be \\(O(\\text{Height of the tree})=O(\\log(N)),\\) where \\(N\\) is the number of nodes in the tree</p>"},{"location":"Week-6/03%20Heaps/#delete_max-or-delete_min_1","title":"<code>delete_max() or delete_min()</code>","text":"<p>If you ponder upon the function, you can see that we follow only a single path, hence the complexity of deleting the root node will be \\(O(\\text{Height of the tree})=O(\\log(N)),\\) where \\(N\\) is the number of nodes in the tree.</p>"}]}